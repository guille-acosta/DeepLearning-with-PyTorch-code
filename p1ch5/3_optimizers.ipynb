{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LZLD2R7Vppli"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WGpkKPBkpplk"
      },
      "outputs": [],
      "source": [
        "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0,\n",
        "                    8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
        "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\n",
        "                    33.9, 21.8, 48.4, 60.4, 68.4])\n",
        "t_un = 0.1 * t_u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KOUgsMJFppll"
      },
      "outputs": [],
      "source": [
        "def model(t_u, w, b):\n",
        "    return w * t_u + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0qn90oDMppll"
      },
      "outputs": [],
      "source": [
        "def loss_fn(t_p, t_c):\n",
        "    squared_diffs = (t_p - t_c)**2\n",
        "    return squared_diffs.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMFlThJMpplm",
        "outputId": "2fb65ea6-3e8c-4bba-8eb9-9bd631f5a6e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASGD',\n",
              " 'Adadelta',\n",
              " 'Adafactor',\n",
              " 'Adagrad',\n",
              " 'Adam',\n",
              " 'AdamW',\n",
              " 'Adamax',\n",
              " 'LBFGS',\n",
              " 'NAdam',\n",
              " 'Optimizer',\n",
              " 'RAdam',\n",
              " 'RMSprop',\n",
              " 'Rprop',\n",
              " 'SGD',\n",
              " 'SparseAdam',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_adafactor',\n",
              " '_functional',\n",
              " 'lr_scheduler',\n",
              " 'swa_utils']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "dir(optim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "85ZIGVZtppln"
      },
      "outputs": [],
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)  # requires_grad=True para habilitar el rastreo de las operaciones sobre params\n",
        "learning_rate = 1e-5\n",
        "optimizer = optim.SGD([params], lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR_Hb2E3ppln",
        "outputId": "180214e3-c7ec-4670-a9c2-8578aa74161c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "t_p = model(t_u, *params)\n",
        "loss = loss_fn(t_p, t_c)\n",
        "loss.backward()\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYOzaiBuppln",
        "outputId": "4e243dde-c35f-4227-fc49-c21406eb5dc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.7761, 0.1064], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "t_p = model(t_un, *params)\n",
        "loss = loss_fn(t_p, t_c)\n",
        "\n",
        "optimizer.zero_grad() # Para que en cada ciclo de entrenamiento se reseteen los gradientes\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o-rbtkGppplo"
      },
      "outputs": [],
      "source": [
        "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t_p = model(t_u, *params)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "\n",
        "        optimizer.zero_grad()  # Reseteo del gradiente\n",
        "        loss.backward()  # Calculo del gradiente en si\n",
        "        optimizer.step()  # Actualización de los parámetros\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQopVUukpplo",
        "outputId": "1046fcae-ab1c-487c-bff5-579ef58c8cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 7.860120\n",
            "Epoch 1000, Loss 3.828538\n",
            "Epoch 1500, Loss 3.092191\n",
            "Epoch 2000, Loss 2.957698\n",
            "Epoch 2500, Loss 2.933134\n",
            "Epoch 3000, Loss 2.928648\n",
            "Epoch 3500, Loss 2.927830\n",
            "Epoch 4000, Loss 2.927679\n",
            "Epoch 4500, Loss 2.927652\n",
            "Epoch 5000, Loss 2.927647\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate) # Instancia del optimizador SGD del submodulo optim\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params, # Mismo argumento con el que se llamó al optimizador\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zhcX1NVpplp",
        "outputId": "3f211e02-1bfc-4dc7-b24d-3aeac42b3670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 7.612900\n",
            "Epoch 1000, Loss 3.086698\n",
            "Epoch 1500, Loss 2.928578\n",
            "Epoch 2000, Loss 2.927646\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  0.5367, -17.3021], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-1  # Notar que se cambió el lr por uno mas grande (y Adam no se hace problema)\n",
        "optimizer = optim.Adam([params], lr=learning_rate) # Se instancia un nuevo Optimizador\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 2000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = t_u, # Adam es menos sencible al escalado de parámetros (no se usa t_un)\n",
        "    t_c = t_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_SW-2tvpplp",
        "outputId": "2d5eada3-ff42-4c07-94b1-47e4e577e4a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([10,  4,  3,  9,  7,  0,  1,  5,  6]), tensor([2, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# División de los datos en dos grupos, entrenamiento y validación\n",
        "\n",
        "n_samples = t_u.shape[0]  # Número total de datos\n",
        "n_val = int(0.2 * n_samples)  # 20% de los datos serán para validación\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)  # Genera una permutación aleatoria de los índices\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_indices, val_indices  # Pueden cambiar de ejecución en ejecución"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4LCKPlc3pplq"
      },
      "outputs": [],
      "source": [
        "train_t_u = t_u[train_indices]\n",
        "train_t_c = t_c[train_indices]\n",
        "\n",
        "val_t_u = t_u[val_indices]\n",
        "val_t_c = t_c[val_indices]\n",
        "\n",
        "train_t_un = 0.1 * train_t_u\n",
        "val_t_un = 0.1 * val_t_u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eRu5pHC0pplq"
      },
      "outputs": [],
      "source": [
        "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
        "                  train_t_c, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = model(train_t_u, *params) # Predicciones del modelo para los datos de entrenamiento\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "        val_t_p = model(val_t_u, *params) # Predicciones para los datos de validación\n",
        "        val_loss = loss_fn(val_t_p, val_t_c)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward() # No hay un val_loss.backward() ya que no queremos entrenar\n",
        "        optimizer.step()      # el modelo con los datos de validación\n",
        "\n",
        "        if epoch <= 3 or epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
        "                  f\" Validation loss {val_loss.item():.4f}\")\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKHSjgyOpplq",
        "outputId": "10b1eb97-fbb4-40c8-a800-99f574e745fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 88.7100, Validation loss 42.8090\n",
            "Epoch 2, Training loss 41.4218, Validation loss 13.5269\n",
            "Epoch 3, Training loss 34.1070, Validation loss 12.9835\n",
            "Epoch 500, Training loss 6.6346, Validation loss 7.8649\n",
            "Epoch 1000, Training loss 3.1442, Validation loss 5.6421\n",
            "Epoch 1500, Training loss 2.6852, Validation loss 4.9298\n",
            "Epoch 2000, Training loss 2.6249, Validation loss 4.6839\n",
            "Epoch 2500, Training loss 2.6170, Validation loss 4.5963\n",
            "Epoch 3000, Training loss 2.6159, Validation loss 4.5647\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3056, -16.8024], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 3000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    train_t_u = train_t_un, # <1>\n",
        "    val_t_u = val_t_un, # <1>\n",
        "    train_t_c = train_t_c,\n",
        "    val_t_c = val_t_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-DxLQ9t8pplr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Para evitar que se cree un grafo computacional al obtener val_t_p y calcular valr_loss (no hace falta\n",
        "ya que no se pretende hacer val_loss.backward) lo que implicaria un desperdicio de recursos sobre todo\n",
        "en modelos grandes, se encapsulan estos calculos en un bloque with\n",
        "\"\"\"\n",
        "\n",
        "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
        "                  train_t_c, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = model(train_t_u, *params)\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_t_p = model(val_t_u, *params)\n",
        "            val_loss = loss_fn(val_t_p, val_t_c)\n",
        "            assert val_loss.requires_grad == False # Comprueba que requires_grad este en False\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xgbnOX9Ypplr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Usando el administrador de contexto set_autograd_enable se puede condicionar el código para que para que se ejecute\n",
        "con autograd habilitado/deshabilitado según se este entrenando el modelo o haciendo inferencia\n",
        "\"\"\"\n",
        "def calc_forward(t_u, t_c, is_train):\n",
        "    with torch.set_grad_enabled(is_train):  # Habilita o deshabilita autograd según el valor de is_train\n",
        "        t_p = model(t_u, *params)           # Si is_train=False -> inferencia/validación\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ciclo de entrenamiento usando la función calc_forward\n",
        "\n",
        "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
        "                  train_t_c, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_loss = calc_forward(train_t_u, train_t_c, is_train=True)  # Calculo de la perdida de entrenamiento con autograd activado\n",
        "\n",
        "        val_loss = calc_forward(val_t_u, val_t_c, is_train=False)  # Perdida de validación con autograd desactivado\n",
        "        assert val_loss.requires_grad == False\n",
        "\n",
        "        # Backprop y actualización de parámetros\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "JCuKqeQw4K0Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 3000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    train_t_u = train_t_un,\n",
        "    val_t_u = val_t_un,\n",
        "    train_t_c = train_t_c,\n",
        "    val_t_c = val_t_c)\n",
        "\n",
        "params"
      ],
      "metadata": {
        "id": "UHnxpJyN-Txe",
        "outputId": "e5cffb7c-c5bf-476c-f517-25f16ce06516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3056, -16.8024], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}